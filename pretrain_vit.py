import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import timm
import tome
import time
import os
import json
from datetime import datetime

import warnings
import sys
import io

# compute FLOPs
from fvcore.nn import FlopCountAnalysis, flop_count_table
FVCORE_AVAILABLE = True


IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]


# ---------- æ•°æ®å¢å¼º ----------
train_transform = transforms.Compose([
    transforms.Resize(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(224, padding=4),
    transforms.ToTensor(),
    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),
])

test_transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),
])


def evaluate(model, dataloader, device):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return correct / total


def train_one_rp(r, p, train_loader, test_loader, device, epochs=30, log_interval=5):
    """
    ä¸ºæŸä¸ª (r, p) é…ç½®è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶è¿”å›è®­ç»ƒæ—¶é—´ã€æµ‹è¯•ç²¾åº¦å’Œè®¡ç®—é‡
    
    Args:
        r: æ¯å±‚è¦åˆå¹¶çš„ token æ•°é‡
        p: å‚ä¸ç›¸ä¼¼åº¦è®¡ç®—çš„ token æ¯”ä¾‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡ (cpu/cuda)
        epochs: è®­ç»ƒè½®æ•°
        log_interval: æ¯éš”å¤šå°‘ä¸ª epoch è¾“å‡ºä¸€æ¬¡çŠ¶æ€
    
    Returns:
        train_time: æ€»è®­ç»ƒæ—¶é—´(ç§’)
        acc: æœ€ç»ˆæµ‹è¯•ç²¾åº¦
        flops: è®¡ç®—é‡ (GFLOPs), å¦‚æœæ— æ³•è®¡ç®—åˆ™ä¸º None
    """
    print(f"\n{'='*60}")
    print(f"Training Config: r={r}, p={p}, epochs={epochs}")
    print(f"{'='*60}")

    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = timm.create_model(
        "vit_tiny_patch16_224",
        pretrained=True,
        num_classes=10
    ).to(device)

    # 2. æ‰“ ToMe è¡¥ä¸
    tome.patch.timm(model)
    model.r = r
    model.p = p

    # è®¡ç®— FLOPs (åªè®¡ç®—ä¸€æ¬¡)
    flops_giga = None
    if FVCORE_AVAILABLE:
        try:
            dummy_input = torch.randn(1, 3, 224, 224).to(device)
            # å±è”½ FLOPs è®¡ç®—æ—¶çš„æ‰€æœ‰è¾“å‡ºï¼ˆåŒ…æ‹¬ unsupported operator ä¿¡æ¯ï¼‰
            old_stdout = sys.stdout
            old_stderr = sys.stderr
            sys.stdout = io.StringIO()
            sys.stderr = io.StringIO()
            
            try:
                flops = FlopCountAnalysis(model, dummy_input)
                total_flops = flops.total()
                flops_giga = total_flops / 1e9
            finally:
                # æ¢å¤æ ‡å‡†è¾“å‡º
                sys.stdout = old_stdout
                sys.stderr = old_stderr
        except Exception as e:
            pass  # é™é»˜å¤±è´¥

    # 3. è®¾ç½®è®­ç»ƒè¶…å‚
    lr = 5e-4
    wd = 0.05
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰- é€‚åˆé•¿æ—¶é—´è®­ç»ƒ
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    # 4. è®­ç»ƒ
    start_time = time.time()
    
    # è®°å½•æ¯ä¸ª epoch çš„æŒ‡æ ‡å†å²
    history = {
        "train_loss": [],
        "train_acc": [],
        "test_acc": [],
        "epochs": []
    }
    
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images = images.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡è®­ç»ƒæŒ‡æ ‡
            running_loss += loss.item()
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®¡ç®—æ¯ä¸ª epoch çš„æŒ‡æ ‡
        avg_loss = running_loss / len(train_loader)
        train_acc = correct / total
        test_acc = evaluate(model, test_loader, device)
        
        # è®°å½•åˆ°å†å²
        history["epochs"].append(epoch + 1)
        history["train_loss"].append(float(avg_loss))
        history["train_acc"].append(float(train_acc))
        history["test_acc"].append(float(test_acc))
        
        # æ¯éš” log_interval ä¸ª epoch è¾“å‡ºçŠ¶æ€
        if (epoch + 1) % log_interval == 0 or epoch == 0 or epoch == epochs - 1:
            elapsed_time = time.time() - start_time
            
            print(f"Epoch [{epoch+1:3d}/{epochs}] | "
                  f"Loss: {avg_loss:.4f} | "
                  f"Train Acc: {train_acc:.4f} | "
                  f"Test Acc: {test_acc:.4f} | "
                  f"Time: {elapsed_time:.1f}s")

    # 5. æœ€ç»ˆè¯„ä¼°
    end_time = time.time()
    train_time = end_time - start_time
    final_acc = evaluate(model, test_loader, device)

    print(f"{'-'*60}")
    print(f"Finished: r={r}, p={p} | "
          f"Time: {train_time:.2f}s | "
          f"Accuracy: {final_acc:.4f} | " 
          f"FLOPs: {flops_giga:.4f} GFLOPs")

    return train_time, final_acc, flops_giga, history


def save_results_to_json(r_list, p_list, results, config):
    """
    ä¿å­˜å®éªŒç»“æœåˆ° JSON æ–‡ä»¶ï¼Œå¹¶åˆ›å»ºç‹¬ç«‹çš„å®éªŒæ–‡ä»¶å¤¹
    results: {(r, p): (train_time, accuracy, flops, history)}
    config: åŒ…å«å®éªŒé…ç½®ä¿¡æ¯çš„å­—å…¸
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åˆ›å»ºå®éªŒä¸“å±æ–‡ä»¶å¤¹
    experiment_dir = f"results/experiment_{timestamp}"
    os.makedirs(experiment_dir, exist_ok=True)
    
    results_data = {
        "timestamp": timestamp,
        "experiment_config": config,  # æ·»åŠ å®éªŒé…ç½®ä¿¡æ¯
        "r_list": r_list,
        "p_list": p_list,
        "results": {}
    }
    
    for (r, p), (t, acc, flops, history) in results.items():
        results_data["results"][f"r{r}_p{p}"] = {
            "train_time": t,
            "accuracy": acc,
            "flops_gflops": flops,
            "history": history
        }
    
    # ä¿å­˜ JSON åˆ°å®éªŒæ–‡ä»¶å¤¹
    json_path = f"{experiment_dir}/results.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, indent=4, ensure_ascii=False)
    
    print(f"\nâœ“ Results saved to: {experiment_dir}/")
    print(f"  - JSON file: {json_path}")
    return json_path, experiment_dir



def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("device:", device)

    # ---------- CIFAR10 æ•°æ®é›† ----------
    train_set = datasets.CIFAR10(
        root="./data",
        train=True,
        download=True,
        transform=train_transform
    )
    test_set = datasets.CIFAR10(
        root="./data",
        train=False,
        download=True,
        transform=test_transform
    )

    from torch.utils.data import Subset
    train_indices = list(range(5000))  # åªç”¨å‰ 5000 å¼ 
    test_indices = list(range(1000))  # åªç”¨å‰ 1000 å¼ 
    train_data = Subset(train_set, train_indices)
    test_data = Subset(test_set, test_indices)

    batch_size = 128
    train_loader = DataLoader(
        train_data,
        batch_size=batch_size,  # Kaggle GPU å¯ä»¥ç”¨æ›´å¤§çš„ batch size
        shuffle=True,
        num_workers=2,  # Kaggle ä¸Šè®¾ç½®ä¸º 2
        pin_memory=True  # GPU è®­ç»ƒæ—¶å¯ç”¨
    )
    test_loader = DataLoader(
        test_data,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    print("Train size:", len(train_data), "Test size:", len(test_data))

    # æœ¬åœ° CPU
    # r_list = [64]
    # p_list = [0.8, 0.6]
    # epochs = 1
    # log_interval = 1
    
    # GPU
    r_list = [8, 16, 32, 64]
    p_list = [1.0, 0.8, 0.6, 0.4, 0.2]
    epochs = 10  # å®Œæ•´è®­ç»ƒ
    log_interval = 1  # æ¯ 5 ä¸ª epoch è¾“å‡ºä¸€æ¬¡
    
    # æ”¶é›†å®éªŒé…ç½®ä¿¡æ¯
    experiment_config = {
        "model": "vit_tiny_patch16_224",
        "dataset": "CIFAR-10",
        "train_size": len(train_data),
        "test_size": len(test_data),
        "batch_size": batch_size,
        "epochs": epochs,
        "learning_rate": 5e-4,
        "weight_decay": 0.05,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "device": str(device),
        "num_workers": 2,
        "image_size": 224,
        "num_classes": 10
    }

    results = {}

    for r in r_list:
        for p in p_list:
            t, acc, flops, history = train_one_rp(r, p, train_loader, test_loader, device, 
                                                   epochs=epochs, log_interval=log_interval)
            results[(r, p)] = (t, acc, flops, history)

    # saving results to JSON
    json_path, experiment_dir = save_results_to_json(r_list, p_list, results, experiment_config)
    print(f"\nğŸ‰ Training complete! Use 'python evaluate.py {json_path}' to visualize results.")

if __name__ == "__main__":
    main()